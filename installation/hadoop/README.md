# Hadoop

For large scale processing and concurrency, a single vertically scaled Spark server is not enough. To address large scale processing, DQ has the ability to push compute to an external Hadoop cluster. This page describes the process by which the DQ Agent can be configured to push DQ jobs to Hadoop.

![](<../../.gitbook/assets/Screenshot 2021-06-21 at 9.05.39 AM.png>)
